import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
import matplotlib.pyplot as plt

(train_featureset,train_labels),(test_featureset,test_labels)=keras.datasets.imdb.load_data(num_words=10000)

def multi_hot_sequences(data,dim):
  result=np.zeros((len(data),dim))
  for i,word in enumerate(data):
      result[i,word]=1.0
  return result
  
  train_featureset=multi_hot_sequences(train_featureset,10000)
test_featureset=multi_hot_sequences(test_featureset,10000)

model=keras.models.Sequential()


#   We add regularization inorder to prevent overfitting by adding penalty to weights. 
model.add(keras.layers.Dense(4,kernel_regularizer=keras.regularizers.l2(0.001),activation='relu',input_shape=(10000,)))
#   Drop layer randomly drops some features to prevent overfitting
model.add(keras.layers.Dropout(0.01))
model.add(keras.layers.Dense(4,kernel_regularizer=keras.regularizers.l2(0.001),activation='relu'))
model.add(keras.layers.Dropout(0.01))
model.add(keras.layers.Dense(1,activation='sigmoid'))

model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])

model.summary()

model.fit(train_featureset,train_labels,epochs=10,validation_data=(test_featureset,test_labels),batch_size=512,verbose=2)
